{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author:\n",
    "    Shenxin Zhan,zhanshenxin135@163.com\n",
    "    \n",
    "Reference:\n",
    "    https://arxiv.org/abs/1706.06978\n",
    "    Deep Interest Network for Click-Through Rate Prediction\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from common import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIN(nn.Module):\n",
    "    def __init__(self, params, use_batchnorm=True, use_dropout=True):\n",
    "        super(DIN, self).__init__()\n",
    "        \n",
    "        self.device = params['device']\n",
    "        self.feature_size = params['feature_size']\n",
    "        self.embedding_size = params['embedding_size']\n",
    "        self.userItemDict = params['userItemDict']\n",
    "        self.hidden_dims = params['hidden_dims']\n",
    "        self.userItemMaxLen = params['userItemMaxLen']\n",
    "        \n",
    "        feature_embeddings = torch.empty(self.feature_size+1, self.embedding_size, \n",
    "                              dtype=torch.float32, device=self.device, \n",
    "                              requires_grad=True)\n",
    "        nn.init.normal_(feature_embeddings)\n",
    "        self.feature_embeddings = nn.Parameter(feature_embeddings)\n",
    "        \n",
    "        self.mlp = MLP(params, use_batchnorm=use_batchnorm, use_dropout=use_dropout)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_dims[-1], 1).to(self.device)\n",
    "\n",
    " \n",
    "    def forward(self, features):\n",
    "        feature_idx = features[\"feature_idx\"]\n",
    "        \n",
    "        uid = feature_idx[:, 0]\n",
    "        feaName_ix = [('item_id', 2), ('author_id', 3), ('music_id', 6)]\n",
    "        feaName_maxlen = [('item_id', 350), ('author_id', 250), ('music_id', 100)]\n",
    "        feaName = ['item_id', 'author_id', 'music_id']\n",
    "        ad_idx = {}\n",
    "        for t in feaName_ix:\n",
    "            ad_idx[t[0]] = feature_idx[:, t[1]]\n",
    "            \n",
    "\n",
    "        hist_idx = self.userItemDict.loc[uid.cpu().numpy()][feaName]\n",
    "    \n",
    "        hist_idx_padded = {}\n",
    "        for temp in feaName_maxlen:\n",
    "            hist_idx_padded[temp[0]] = pad_sequence(list(hist_idx[temp[0]]), batch_first=True, \n",
    "                                                 padding_value=self.feature_size)[:, 0:temp[1]].to(self.device)\n",
    "        user_beha_embeddings = []\n",
    "        for temp in feaName:\n",
    "            hist_embeddings = self.feature_embeddings[hist_idx_padded[temp], :] \n",
    "            ad_embeddings = self.feature_embeddings[ad_idx[temp], :]\n",
    "            \n",
    "            hist_weight = torch.einsum('blk,bk->bl', (hist_embeddings, ad_embeddings))\n",
    "            mask = hist_idx_padded[temp] != self.feature_size\n",
    "            hist_weight.masked_fill_(mask == 0, -1e9)\n",
    "            hist_weight = torch.softmax(hist_weight, dim=1)\n",
    "            user_beha_embeddings.append(torch.einsum('blk,bl->bk', (hist_embeddings, hist_weight)))\n",
    "        \n",
    "        user_beha_embeddings = torch.cat(user_beha_embeddings, dim=1)\n",
    "        \n",
    "        ad_embeddings = self.feature_embeddings[feature_idx, :].reshape(feature_idx.shape[0], -1)\n",
    "        \n",
    "        embeddings = torch.cat((user_beha_embeddings, ad_embeddings), dim=1)\n",
    "        \n",
    "        # deep\n",
    "#         deepInput = embeddings.reshape(embeddings.shape[0], self.mlp_input_dim)\n",
    "        deepOut = self.mlp(embeddings)\n",
    "        \n",
    "        logits = self.output_layer(deepOut)\n",
    "        \n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
